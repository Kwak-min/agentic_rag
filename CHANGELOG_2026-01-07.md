# λ³€κ²½ μ΄λ ¥ - 2026λ…„ 1μ›” 7μΌ

## π― μ£Όμ” λ³€κ²½μ‚¬ν•­

### 1. AI λ¨λΈ ν†µν•©: μ „μ²΄ μ‹μ¤ν…μ„ Ollama qwen2.5:7bλ΅ ν†µμΌ

**μ΄μ „ κµ¬μ„±:**
- μΌλ° λ€ν™”: LM Studio
- λ„κµ¬ μ‘λ‹µ: LM Studio
- μμ¨ μ—μ΄μ „νΈ (νν”„ μ μ–΄): LM Studio

**λ³€κ²½ ν›„:**
- μΌλ° λ€ν™”: **Ollama qwen2.5:7b**
- λ„κµ¬ μ‘λ‹µ: **Ollama qwen2.5:7b**
- μμ¨ μ—μ΄μ „νΈ (νν”„ μ μ–΄): **Ollama qwen2.5:7b**

---

## π“ μμ •λ νμΌ

### 1. **ν™κ²½ μ„¤μ • νμΌ**

#### `.env`
```bash
# μ¶”κ°€/μμ •λ μ„¤μ •
OLLAMA_BASE_URL=http://host.docker.internal:11434
OLLAMA_MODEL_NAME=qwen2.5:7b
USE_OLLAMA=true
SKIP_EMBEDDING_LOAD=true
```

#### `docker-compose-new.yml`
```yaml
environment:
  OLLAMA_BASE_URL: http://host.docker.internal:11434
  OLLAMA_MODEL_NAME: qwen2.5:7b
  SKIP_EMBEDDING_LOAD: true
```

#### `config.py` (35-37ν–‰)
```python
# Ollama μ„¤μ • μ¶”κ°€
USE_OLLAMA = os.getenv("USE_OLLAMA", "true").lower() == "true"
OLLAMA_BASE_URL = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
OLLAMA_MODEL_NAME = os.getenv("OLLAMA_MODEL_NAME", "qwen2.5:7b")
```

---

### 2. **λ¨λΈ ν΄λΌμ΄μ–ΈνΈ νμΌ**

#### `models/ollama_client.py` (31ν–‰)
```python
# κΈ°λ³Έ λ¨λΈ λ³€κ²½
self.model = model_name or os.getenv("OLLAMA_MODEL_NAME", "qwen2.5:7b")
```

---

### 3. **λ°±μ—”λ“ μ΄κΈ°ν™” νμΌ**

#### `flask_app.py` (20-27, 51-59ν–‰)
```python
# Ollama ν΄λΌμ΄μ–ΈνΈ import μ¶”κ°€
from models.ollama_client import OllamaClient
from config import USE_OLLAMA, OLLAMA_BASE_URL, OLLAMA_MODEL_NAME

# μ΄κΈ°ν™” λ΅μ§ μμ •
if USE_OLLAMA:
    lm_client = OllamaClient(base_url=OLLAMA_BASE_URL, model_name=OLLAMA_MODEL_NAME)
else:
    lm_client = LMStudioClient()
```

#### `flask_app.py` (138-147ν–‰)
```python
# API μ—”λ“ν¬μΈνΈ μμ • (async λ¬Έμ  ν•΄κ²°)
result = orchestrator.process_query_sync(user_message, stream=False)
```

#### `flask_app.py` (305-325ν–‰)
```python
# SocketIO μ¤νΈλ¦¬λ° μμ •
stream_generator = orchestrator.process_query_sync(user_message, stream=True)
for chunk_data in stream_generator:
    if chunk_data.get('type') == 'chunk':
        emit('chat_chunk', {'chunk': chunk_data.get('content', '')})
```

---

### 4. **Streamlit μ•±**

#### `app.py` (11, 15, 171-179ν–‰)
```python
# Ollama μ§€μ› μ¶”κ°€
from models.ollama_client import OllamaClient
from config import USE_OLLAMA, OLLAMA_BASE_URL, OLLAMA_MODEL_NAME

# μ΄κΈ°ν™” λ΅μ§
if USE_OLLAMA:
    lm_studio_client = OllamaClient(base_url=OLLAMA_BASE_URL, model_name=OLLAMA_MODEL_NAME)
else:
    lm_studio_client = LMStudioClient()
```

---

### 5. **μμ¨ μ—μ΄μ „νΈ (ν•µμ‹¬ λ³€κ²½)**

#### `services/autonomous_agent.py`

**μ£Όμ„ λ° νƒ€μ… ννΈ μμ •:**
```python
# 1ν–‰: μ£Όμ„ λ³€κ²½
# services/autonomous_agent.py - AI κΈ°λ° μμ¨ν• μ—μ΄μ „νΈ

# 12ν–‰: LMStudioClient import μ κ±°
# from models.lm_studio import LMStudioClient  # μ κ±°λ¨

# 52ν–‰: docstring μμ •
"""AI κΈ°λ° μμ¨ν• μ—μ΄μ „νΈ (Ollama/LM Studio μ§€μ›)"""

# 59-63ν–‰: νƒ€μ… ννΈ μ κ±° λ° docstring μ¶”κ°€
def __init__(self, lm_client):
    """
    Args:
        lm_client: AI ν΄λΌμ΄μ–ΈνΈ (OllamaClient λλ” LMStudioClient)
    """
```

**AI νΈμ¶ λ΅μ§ μμ • (296-332ν–‰):**
```python
# μ΄μ „: LM Studio OpenAI νΈν™ APIλ§ μ§€μ›
response = self.lm_client.client.chat.completions.create(...)

# λ³€κ²½ ν›„: Ollamaμ™€ LM Studio λ¨λ‘ μ§€μ›
if hasattr(self.lm_client, 'chat_completion'):
    # Ollama ν΄λΌμ΄μ–ΈνΈ μ‚¬μ©
    messages = [
        {"role": "system", "content": self.system_prompt},
        {"role": "user", "content": user_message},
    ]
    ai_response = self.lm_client.chat_completion(
        messages=messages,
        temperature=0.3,
        stream=False
    )
elif hasattr(self.lm_client, 'client'):
    # LM Studio ν΄λΌμ΄μ–ΈνΈ μ‚¬μ© (OpenAI νΈν™)
    response = self.lm_client.client.chat.completions.create(...)
    ai_response = response.choices[0].message.content if response else None
```

**μ „μ—­ ν•¨μ μμ • (849-858ν–‰):**
```python
def get_autonomous_agent(lm_client=None) -> Optional[AutonomousAgent]:
    """μ „μ—­ μμ¨ μ—μ΄μ „νΈ μΈμ¤ν„΄μ¤ λ°ν™

    Args:
        lm_client: AI ν΄λΌμ΄μ–ΈνΈ (OllamaClient λλ” LMStudioClient)
    """
```

---

### 6. **λ¬Έμ„ν™”**

#### μ‹ κ· νμΌ μƒμ„±
- `PROMPTS.md` - ν”„λ΅¬ν”„νΈ μƒμ„Έ κ°€μ΄λ“ (12KB)
- `PROMPTS_QUICK_REFERENCE.md` - ν”„λ΅¬ν”„νΈ λΉ λ¥Έ μ°Έμ΅° (4.8KB)

#### μμ •λ λ¬Έμ„
- `PROMPTS.md` - μμ¨ μ—μ΄μ „νΈ λ¨λΈμ„ "Ollama qwen2.5:7b"λ΅ μ—…λ°μ΄νΈ
- `PROMPTS_QUICK_REFERENCE.md` - λ¨λΈ μ •λ³΄ ν‘ μ—…λ°μ΄νΈ

---

## π”§ κΈ°μ μ  κ°μ„ μ‚¬ν•­

### 1. **λ™κΈ°/λΉ„λ™κΈ° λ¬Έμ  ν•΄κ²°**
- **λ¬Έμ **: Flask μ—”λ“ν¬μΈνΈμ—μ„ `async def process_query()`λ¥Ό μ§μ ‘ νΈμ¶
- **ν•΄κ²°**: `process_query_sync()` λ©”μ„λ“ μ‚¬μ©μΌλ΅ λ³€κ²½

### 2. **ν΄λΌμ΄μ–ΈνΈ ν†µν•©**
- **μ΄μ „**: LM Studio μ „μ© μ½”λ“
- **λ³€κ²½**: Duck typingμ„ ν™μ©ν• λ‹¤μ¤‘ ν΄λΌμ΄μ–ΈνΈ μ§€μ›
  - `hasattr(client, 'chat_completion')` β†’ Ollama
  - `hasattr(client, 'client')` β†’ LM Studio

### 3. **μ„λ² λ”© λ΅λ”© μµμ ν™”**
- `SKIP_EMBEDDING_LOAD=true` μ„¤μ •μΌλ΅ μ‹μ‘ μ‹κ°„ λ‹¨μ¶•

---

## β… ν…μ¤νΈ κ²°κ³Ό

### λ°±μ—”λ“ μƒνƒ
```
β… PostgreSQL: μ •μƒ μ‹¤ν–‰ (ν¬νΈ 5432)
β… Backend (Flask): μ •μƒ μ‹¤ν–‰ (ν¬νΈ 5000)
β… Frontend (Next.js): μ •μƒ μ‹¤ν–‰ (ν¬νΈ 3000)
β… Ollama ν΄λΌμ΄μ–ΈνΈ: qwen2.5:7bλ΅ μ΄κΈ°ν™” μ„±κ³µ
```

### API ν…μ¤νΈ
```bash
# ν—¬μ¤μ²΄ν¬
$ curl http://localhost:5000/api/health
{"status": "healthy", "system_initialized": true}

# μ‹μ¤ν… μƒνƒ
$ curl http://localhost:5000/api/system/status
# μ •μƒ μ‘λ‹µ ν™•μΈ
```

---

## β οΈ μ•λ ¤μ§„ μ΄μ λ° ν•΄κ²°

### μ΄μ 1: Ollama 404 μ—λ¬
**μ›μΈ**: qwen2.5:7b λ¨λΈμ΄ λ΅μ»¬μ— λ‹¤μ΄λ΅λ“λμ§€ μ•μ

**ν•΄κ²° λ°©λ²•**:
```bash
ollama pull qwen2.5:7b
```

**μƒνƒ**:
- Ollama μ„λ²„λ” μ •μƒ μ‘λ™ μ¤‘
- phi3:mini λ¨λΈλ§ μ„¤μΉλμ–΄ μμ
- μ‚¬μ©μκ°€ qwen2.5:7b λ‹¤μ΄λ΅λ“ ν•„μ”

### μ΄μ 2: ν•κΈ€ μΈμ½”λ”©
**μƒνƒ**: Windows ν„°λ―Έλ„μ—μ„ ν•κΈ€ κΉ¨μ§ (μ„λ²„λ” μ •μƒ)
**μν–¥**: μ—†μ (μ›Ή UIμ—μ„λ” μ •μƒ ν‘μ‹)

---

## π€ λ°°ν¬ μ²΄ν¬λ¦¬μ¤νΈ

- [x] ν™κ²½ λ³€μ μ„¤μ • μ™„λ£
- [x] Docker Compose νμΌ μ—…λ°μ΄νΈ
- [x] λ°±μ—”λ“ μ΄κΈ°ν™” λ΅μ§ μμ •
- [x] μμ¨ μ—μ΄μ „νΈ ν†µν•©
- [x] λ¬Έμ„ν™” μ™„λ£
- [x] λ°±μ—”λ“ μ¬μ‹μ‘ λ° ν…μ¤νΈ
- [ ] qwen2.5:7b λ¨λΈ λ‹¤μ΄λ΅λ“ (μ‚¬μ©μ μ‘μ—…)
- [ ] μ›Ή UIμ—μ„ μ±„ν… κΈ°λ¥ ν…μ¤νΈ

---

## π“ μ°Έκ³  λ¬Έμ„

- [PROMPTS.md](PROMPTS.md) - μ „μ²΄ ν”„λ΅¬ν”„νΈ κ°€μ΄λ“
- [PROMPTS_QUICK_REFERENCE.md](PROMPTS_QUICK_REFERENCE.md) - λΉ λ¥Έ μ°Έμ΅°
- [MIGRATION_GUIDE.md](MIGRATION_GUIDE.md) - LM Studio β†’ Ollama λ§μ΄κ·Έλ μ΄μ… κ°€μ΄λ“

---

## π”„ λ΅¤λ°± λ°©λ²•

λ³€κ²½ μ‚¬ν•­μ„ λλλ ¤μ•Ό ν•  κ²½μ°:

```bash
# 1. GitμΌλ΅ λ³µμ› (μ»¤λ°‹ μ „μ΄λΌλ©΄)
git checkout -- .env config.py flask_app.py app.py services/autonomous_agent.py

# 2. λ°±μ—… νμΌ λ³µμ› (λ°±μ—…μ΄ μλ‹¤λ©΄)
cp .env.backup .env
cp services/autonomous_agent.py.backup services/autonomous_agent.py

# 3. μ»¨ν…μ΄λ„ μ¬μ‹μ‘
docker-compose -f docker-compose-new.yml restart backend
```

---

## π‘¤ μ‘μ„±μ

- λ‚ μ§: 2026-01-07
- λ³€κ²½ μ‚¬μ : λ¨λ“  AI κΈ°λ¥μ„ Ollama qwen2.5:7bλ΅ ν†µμΌν•μ—¬ μ‹μ¤ν… μΌκ΄€μ„± ν™•λ³΄
- μν–¥ λ²”μ„: μ „μ²΄ μ‹μ¤ν… (μΌλ° λ€ν™”, λ„κµ¬ μ‘λ‹µ, μμ¨ μ—μ΄μ „νΈ)

---

## π“ ν†µκ³„

- μμ •λ νμΌ: 8κ°
- μ¶”κ°€λ νμΌ: 3κ° (λ¬Έμ„)
- μ΄ λ³€κ²½ λΌμΈ: μ•½ 200μ¤„
- μ‚­μ λ μμ΅΄μ„±: 0κ° (LMStudioClientλ” νΈν™μ„±μ„ μ„ν•΄ μ μ§€)
